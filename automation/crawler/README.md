# ê¸°ìˆ  ë¸”ë¡œê·¸ í¬ë¡¤ë§ ì‹œìŠ¤í…œ ğŸ“°

## ğŸ“‹ ê°œìš”

ì´ ì‹œìŠ¤í…œì€ ë§¤ì¼ ìë™ìœ¼ë¡œ ì£¼ìš” ê¸°ìˆ  ë¸”ë¡œê·¸ë“¤ì„ í¬ë¡¤ë§í•˜ì—¬ ìµœì‹  ê¸°ìˆ  ë™í–¥ì„ ìˆ˜ì§‘í•˜ê³  AIë¡œ ìš”ì•½í•˜ëŠ” ìë™í™” ë„êµ¬ì…ë‹ˆë‹¤.

### ğŸ¯ ì£¼ìš” ê¸°ëŠ¥

1. **24ì‹œê°„ ê¸°ì¤€ ì»¨í…ì¸  ìˆ˜ì§‘** - ì§€ë‚œ 24ì‹œê°„ ë™ì•ˆ ë°œí–‰ëœ ê¸€ë§Œ ì„ ë³„ì ìœ¼ë¡œ í¬ë¡¤ë§
2. **ë‹¤ì¤‘ ì†ŒìŠ¤ ì§€ì›** - RSS, API, HTML íŒŒì‹±ì„ í†µí•œ ë‹¤ì–‘í•œ ë¸”ë¡œê·¸ ì§€ì›
3. **AI ê¸°ë°˜ ìš”ì•½** - OpenAI GPT-4, Anthropic Claudeë¥¼ í™œìš©í•œ ìë™ ìš”ì•½
4. **GitHub ì €ì¥** - DB ì—†ì´ GitHub ì €ì¥ì†Œì— êµ¬ì¡°í™”ëœ ë°ì´í„° ë³´ê´€
5. **ì‹¤ì‹œê°„ ì•Œë¦¼** - Slack, Telegramì„ í†µí•œ í¬ë¡¤ë§ ê²°ê³¼ ì•Œë¦¼

---

## ğŸ” í¬ë¡¤ë§ ëŒ€ìƒ ë° ì„ ì • ê¸°ì¤€

### ğŸ“š í¬ë¡¤ë§ ëŒ€ìƒ ë¸”ë¡œê·¸

#### ğŸŒ ì˜ì–´ ê¸°ìˆ  ë¸”ë¡œê·¸ (3ê³³)

1. **Hacker News** ğŸ”¥
   - **URL**: https://news.ycombinator.com/
   - **ë°©ì‹**: RSS í”¼ë“œ (`https://hnrss.org/frontpage`)
   - **íŠ¹ì§•**: ê°œë°œì ì»¤ë®¤ë‹ˆí‹°ì˜ í•«í•œ ì†Œì‹, ë†’ì€ ì‹ ë¢°ë„
   - **ìˆ˜ì§‘ ê¸°ì¤€**: ìŠ¤ì½”ì–´ 100 ì´ìƒ, ëŒ“ê¸€ 20ê°œ ì´ìƒ

2. **Dev.to** ğŸ’»
   - **URL**: https://dev.to/
   - **ë°©ì‹**: REST API (`https://dev.to/api/articles`)
   - **íŠ¹ì§•**: ê°œë°œì ì¤‘ì‹¬ ë¸”ë¡œê·¸ í”Œë«í¼, ë‹¤ì–‘í•œ ê¸°ìˆ  ìŠ¤íƒ
   - **ìˆ˜ì§‘ ê¸°ì¤€**: ì¢‹ì•„ìš” 50ê°œ ì´ìƒ, íƒœê·¸ 3ê°œ ì´ìƒ

3. **Medium Engineering** ğŸ¢
   - **URL**: https://medium.engineering/
   - **ë°©ì‹**: RSS í”¼ë“œ
   - **íŠ¹ì§•**: ëŒ€ê¸°ì—… ì—”ì§€ë‹ˆì–´ë§ íŒ€ì˜ ì‹¬í™” ê¸°ìˆ  ê¸€
   - **ìˆ˜ì§‘ ê¸°ì¤€**: ë°•ìˆ˜ 100ê°œ ì´ìƒ, ì½ê¸° ì‹œê°„ 5ë¶„ ì´ìƒ

#### ğŸ‡°ğŸ‡· í•œêµ­ì–´ ê¸°ìˆ  ë¸”ë¡œê·¸ (3ê³³)

1. **ì¹´ì¹´ì˜¤ ê¸°ìˆ ë¸”ë¡œê·¸** ğŸ¨
   - **URL**: https://tech.kakao.com/
   - **ë°©ì‹**: RSS í”¼ë“œ (`https://tech.kakao.com/feed/`)
   - **íŠ¹ì§•**: ì¹´ì¹´ì˜¤ì˜ ëŒ€ê·œëª¨ ì„œë¹„ìŠ¤ ìš´ì˜ ë…¸í•˜ìš°
   - **ìˆ˜ì§‘ ê¸°ì¤€**: ëª¨ë“  ìƒˆ ê¸€ (ë†’ì€ í’ˆì§ˆ ë³´ì¥)

2. **ìš°ì•„í•œí˜•ì œë“¤ ê¸°ìˆ ë¸”ë¡œê·¸** ğŸ•
   - **URL**: https://techblog.woowahan.com/
   - **ë°©ì‹**: RSS í”¼ë“œ
   - **íŠ¹ì§•**: ë°°ë‹¬ì˜ë¯¼ì¡± ì„œë¹„ìŠ¤ ê°œë°œ ê²½í—˜, ì‹¤ë¬´ ì¤‘ì‹¬
   - **ìˆ˜ì§‘ ê¸°ì¤€**: ëª¨ë“  ìƒˆ ê¸€ (ì—„ì„ ëœ ì»¨í…ì¸ )

3. **ë„¤ì´ë²„ D2** ğŸ”
   - **URL**: https://d2.naver.com/
   - **ë°©ì‹**: API (ì»¤ìŠ¤í…€)
   - **íŠ¹ì§•**: ë„¤ì´ë²„ì˜ ê¸°ìˆ  ì—°êµ¬ ë° ì˜¤í”ˆì†ŒìŠ¤ í”„ë¡œì íŠ¸
   - **ìˆ˜ì§‘ ê¸°ì¤€**: ì¡°íšŒìˆ˜ 1000 ì´ìƒ, ê¸°ìˆ  ì¹´í…Œê³ ë¦¬

### ğŸ“Š ì»¨í…ì¸  ì„ ì • ê¸°ì¤€

#### 1. ì‹œê°„ ê¸°ì¤€ ğŸ“…
```python
# 24ì‹œê°„ ê¸°ì¤€ í•„í„°ë§
cutoff_date = datetime.now() - timedelta(days=1)
```

- **ê¸°ë³¸ ì›ì¹™**: ì§€ë‚œ 24ì‹œê°„ ë™ì•ˆ ë°œí–‰ëœ ê¸€ë§Œ ìˆ˜ì§‘
- **ì˜ˆì™¸ ì²˜ë¦¬**: ì£¼ë§ì´ë‚˜ ê³µíœ´ì¼ ê³ ë ¤í•˜ì—¬ ìµœëŒ€ 48ì‹œê°„ê¹Œì§€ í™•ì¥ ê°€ëŠ¥
- **íƒ€ì„ì¡´**: UTC ê¸°ì¤€ìœ¼ë¡œ í†µì¼í•˜ì—¬ ì²˜ë¦¬

#### 2. í’ˆì§ˆ ê¸°ì¤€ â­
```python
def validate_article(article):
    # ìµœì†Œ ì»¨í…ì¸  ê¸¸ì´
    if len(article.get('content', '')) < 100:
        return False
    
    # ê¸°ìˆ  ê´€ë ¨ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€
    tech_keywords = ['programming', 'development', 'software', ...]
    if not any(keyword in article['content'].lower() for keyword in tech_keywords):
        return False
    
    return True
```

**í•„ìˆ˜ ì¡°ê±´:**
- ê¸€ ì œëª©ê³¼ URL ì¡´ì¬
- ì»¨í…ì¸  ê¸¸ì´ 100ì ì´ìƒ
- ê¸°ìˆ  ê´€ë ¨ í‚¤ì›Œë“œ 1ê°œ ì´ìƒ í¬í•¨

**ê°€ì‚°ì  ê¸°ì¤€:**
- ì½”ë“œ ìŠ¤ë‹ˆí« í¬í•¨ (+ì ìˆ˜)
- ì´ë¯¸ì§€/ë‹¤ì´ì–´ê·¸ë¨ í¬í•¨ (+ì ìˆ˜)
- ì™¸ë¶€ ë§í¬ ë° ì°¸ê³ ìë£Œ (+ì ìˆ˜)

#### 3. ì¸ê¸°ë„ ê¸°ì¤€ ğŸ“ˆ

**í”Œë«í¼ë³„ ê¸°ì¤€:**
- **Hacker News**: ìŠ¤ì½”ì–´ 50+ ë˜ëŠ” ëŒ“ê¸€ 10+
- **Dev.to**: í•˜íŠ¸ 30+ ë˜ëŠ” ë¶ë§ˆí¬ 10+
- **Medium**: ë°•ìˆ˜ 50+ ë˜ëŠ” í•˜ì´ë¼ì´íŠ¸ 5+
- **í•œêµ­ ë¸”ë¡œê·¸**: ëª¨ë“  ìƒˆ ê¸€ (ìì²´ í’ˆì§ˆ ê´€ë¦¬)

#### 4. ì¹´í…Œê³ ë¦¬ í•„í„° ğŸ·ï¸
```python
ALLOWED_CATEGORIES = [
    'programming', 'software-development', 'web-development',
    'mobile-development', 'devops', 'cloud-computing',
    'machine-learning', 'artificial-intelligence', 'data-science',
    'cybersecurity', 'blockchain', 'open-source'
]
```

---

## ğŸ”§ í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤

### 1. ë°ì´í„° ìˆ˜ì§‘ ë‹¨ê³„

```mermaid
graph TD
    A[ìŠ¤ì¼€ì¤„ íŠ¸ë¦¬ê±°] --> B[ë¸”ë¡œê·¸ ëª©ë¡ í™•ì¸]
    B --> C{RSS í”¼ë“œ?}
    C -->|Yes| D[RSS íŒŒì‹±]
    C -->|No| E{API ì‚¬ìš©?}
    E -->|Yes| F[API í˜¸ì¶œ]
    E -->|No| G[HTML íŒŒì‹±]
    D --> H[ë‚ ì§œ í•„í„°ë§]
    F --> H
    G --> H
    H --> I[í’ˆì§ˆ ê²€ì¦]
    I --> J[ì¤‘ë³µ ì œê±°]
    J --> K[ì „ì²´ ì»¨í…ì¸  ê°€ì ¸ì˜¤ê¸°]
```

#### A. RSS í”¼ë“œ í¬ë¡¤ë§
```python
async def crawl_from_rss(blog_name, config, cutoff_date):
    # 1. RSS í”¼ë“œ ìš”ì²­
    async with session.get(config["rss_feed"]) as response:
        rss_content = await response.text()
    
    # 2. íŒŒì‹± ë° í•„í„°ë§
    feed = feedparser.parse(rss_content)
    for entry in feed.entries:
        if parse_date(entry.published) > cutoff_date:
            # ì²˜ë¦¬ ë¡œì§
```

#### B. API ê¸°ë°˜ í¬ë¡¤ë§
```python
async def crawl_from_api(blog_name, config, cutoff_date):
    # 1. API í˜¸ì¶œ (í˜ì´ì§€ë„¤ì´ì…˜ ê³ ë ¤)
    url = f"{config['api_endpoint']}?per_page=50&since={cutoff_date}"
    
    # 2. ì‘ë‹µ ë°ì´í„° ì²˜ë¦¬
    async with session.get(url) as response:
        data = await response.json()
        # ë°ì´í„° ë³€í™˜ ë° í•„í„°ë§
```

#### C. HTML íŒŒì‹± í¬ë¡¤ë§
```python
async def crawl_from_html(blog_name, config, cutoff_date):
    # 1. ë©”ì¸ í˜ì´ì§€ ë¡œë“œ
    async with session.get(config["url"]) as response:
        html = await response.text()
    
    # 2. BeautifulSoupìœ¼ë¡œ íŒŒì‹±
    soup = BeautifulSoup(html, 'html.parser')
    
    # 3. ì…€ë ‰í„° ê¸°ë°˜ ë°ì´í„° ì¶”ì¶œ
    articles = soup.select(config["selectors"]["article"])
```

### 2. ë°ì´í„° ê²€ì¦ ë‹¨ê³„

```python
def validate_articles(articles, cutoff_date):
    validated = []
    for article in articles:
        # í•„ìˆ˜ í•„ë“œ ê²€ì¦
        if not article.get('title') or not article.get('url'):
            continue
            
        # ì»¨í…ì¸  ê¸¸ì´ ê²€ì¦
        if len(article.get('content', '')) < 100:
            continue
            
        # ë‚ ì§œ ê²€ì¦
        published_date = parse_date(article.get('published_date'))
        if published_date < cutoff_date:
            continue
            
        validated.append(article)
    
    return validated
```

### 3. AI ìš”ì•½ ë‹¨ê³„

```python
async def summarize_article(article):
    prompt = f"""
    ë‹¤ìŒ ê¸°ìˆ  ë¸”ë¡œê·¸ ê¸€ì„ 200ì ì´ë‚´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:
    
    ì œëª©: {article['title']}
    ë‚´ìš©: {article['content'][:2000]}
    
    ìš”ì•½ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ í¬í•¨í•´ì£¼ì„¸ìš”:
    1. í•µì‹¬ ê¸°ìˆ ì´ë‚˜ ê°œë…
    2. ì£¼ìš” ë‚´ìš© ë˜ëŠ” ê²°ë¡ 
    3. ì‹¤ë¬´ ì ìš© ê°€ëŠ¥ì„±
    """
    
    # OpenAI ë˜ëŠ” Claude API í˜¸ì¶œ
    summary = await ai_client.generate_summary(prompt)
    return summary
```

---

## ğŸ’¾ ë°ì´í„° ì €ì¥ êµ¬ì¡°

### GitHub ì €ì¥ ë°©ì‹

DBë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  GitHub ì €ì¥ì†Œì— êµ¬ì¡°í™”ëœ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

```
research/trends/
â”œâ”€â”€ index.json                 # ì „ì²´ ì¸ë±ìŠ¤
â”œâ”€â”€ 2024-01-15/               # ë‚ ì§œë³„ í´ë”
â”‚   â”œâ”€â”€ crawling_results.json # ì¢…í•© ê²°ê³¼
â”‚   â”œâ”€â”€ Hacker_News.json      # ë¸”ë¡œê·¸ë³„ ê°œë³„ íŒŒì¼
â”‚   â”œâ”€â”€ Dev.to.json
â”‚   â”œâ”€â”€ Medium_Engineering.json
â”‚   â”œâ”€â”€ ì¹´ì¹´ì˜¤_ê¸°ìˆ ë¸”ë¡œê·¸.json
â”‚   â”œâ”€â”€ ìš°ì•„í•œí˜•ì œë“¤_ê¸°ìˆ ë¸”ë¡œê·¸.json
â”‚   â””â”€â”€ ë„¤ì´ë²„_D2.json
â”œâ”€â”€ 2024-01-16/
â””â”€â”€ ...
```

### ğŸ“„ íŒŒì¼ êµ¬ì¡°

#### 1. index.json (ì „ì²´ ì¸ë±ìŠ¤)
```json
{
  "crawling_history": [
    {
      "date": "2024-01-15",
      "total_articles": 23,
      "blogs_crawled": 6,
      "trending_topics": 8,
      "timestamp": "2024-01-15T09:00:00Z"
    }
  ],
  "statistics": {
    "total_crawls": 30,
    "total_articles_collected": 687,
    "last_updated": "2024-01-15T09:00:00Z",
    "average_articles_per_day": 22.9
  }
}
```

#### 2. crawling_results.json (ì¼ì¼ ì¢…í•©)
```json
{
  "date": "2024-01-15",
  "total_articles": 23,
  "blogs": {
    "Hacker News": {
      "source_url": "https://news.ycombinator.com/",
      "articles": [...],
      "count": 8
    }
  },
  "trending_topics": [
    {
      "topic": "AI/Machine Learning",
      "count": 12,
      "description": "AI ëª¨ë¸ ìµœì í™” ë° ì‹¤ë¬´ ì ìš© ì‚¬ë¡€"
    }
  ],
  "summary": "ì˜¤ëŠ˜ì€ AI/ML ê´€ë ¨ ê¸€ì´ ë§ì´ ë°œí–‰ë˜ì—ˆìœ¼ë©°...",
  "metadata": {
    "crawl_start_time": "2024-01-15T09:00:00Z",
    "crawl_duration": "00:05:23",
    "errors": []
  }
}
```

#### 3. ê°œë³„ ë¸”ë¡œê·¸ íŒŒì¼ (ì˜ˆ: Hacker_News.json)
```json
{
  "blog_name": "Hacker News",
  "crawl_date": "2024-01-15",
  "articles": [
    {
      "title": "Building Scalable AI Systems",
      "url": "https://example.com/article1",
      "published_date": "2024-01-15T08:30:00Z",
      "author": "John Doe",
      "summary": "RSSì—ì„œ ê°€ì ¸ì˜¨ ìš”ì•½",
      "content": "ì „ì²´ ê¸€ ë‚´ìš©...",
      "ai_summary": "AIê°€ ìƒì„±í•œ 200ì ìš”ì•½",
      "keywords": ["ai", "scalability", "system-design"],
      "metrics": {
        "score": 156,
        "comments": 42,
        "word_count": 1250
      },
      "language": "en",
      "source": "Hacker News"
    }
  ],
  "metadata": {
    "total_articles": 8,
    "crawl_method": "rss",
    "processing_time": "00:01:15"
  }
}
```

---

## ğŸš€ ì‚¬ìš©ë²•

### 1. ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
# 1. ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r automation/crawler/requirements.txt

# 2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export OPENAI_API_KEY="your-api-key"
export CLAUDE_API_KEY="your-api-key"

# 3. í…ŒìŠ¤íŠ¸ ì‹¤í–‰
cd automation/crawler
python main.py
```

### 2. ìˆ˜ë™ í¬ë¡¤ë§ ì‹¤í–‰

```python
import asyncio
from crawler import TechBlogCrawler
from config import BLOG_SOURCES

async def manual_crawl():
    async with TechBlogCrawler() as crawler:
        # íŠ¹ì • ë¸”ë¡œê·¸ë§Œ í¬ë¡¤ë§
        blog_config = BLOG_SOURCES["Hacker News"]
        articles = await crawler.crawl_blog("Hacker News", blog_config)
        print(f"ìˆ˜ì§‘ëœ ê¸€: {len(articles)}ê°œ")

asyncio.run(manual_crawl())
```

### 3. ì»¤ìŠ¤í…€ ë¸”ë¡œê·¸ ì¶”ê°€

```python
# config.pyì— ì¶”ê°€
BLOG_SOURCES["ìƒˆë¡œìš´ ë¸”ë¡œê·¸"] = {
    "url": "https://example-tech-blog.com",
    "type": "rss",  # ë˜ëŠ” "api", "html"
    "rss_feed": "https://example-tech-blog.com/feed",
    "selectors": {
        "title": ".post-title",
        "content": ".post-content",
        "author": ".author-name",
        "date": ".publish-date"
    },
    "language": "ko",
    "description": "ìƒˆë¡œìš´ ê¸°ìˆ  ë¸”ë¡œê·¸"
}
```

### 4. í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ ë³€ê²½

```yaml
# .github/workflows/daily-crawler.yml
on:
  schedule:
    # ë§¤ì¼ ì˜¤í›„ 2ì‹œ (UTC 5ì‹œ)ë¡œ ë³€ê²½
    - cron: '0 5 * * *'
    # ì£¼ 3íšŒ (ì›”, ìˆ˜, ê¸ˆ)ë¡œ ë³€ê²½
    - cron: '0 0 * * 1,3,5'
```

---

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ë° ë¶„ì„

### 1. í¬ë¡¤ë§ ì„±ê³µë¥  í™•ì¸

```python
# ìµœê·¼ 7ì¼ê°„ ì„±ê³µë¥  ê³„ì‚°
def calculate_success_rate():
    with open('research/trends/index.json') as f:
        data = json.load(f)
    
    recent_crawls = data['crawling_history'][:7]
    total_attempts = len(recent_crawls) * 6  # 6ê°œ ë¸”ë¡œê·¸
    successful_crawls = sum(entry['blogs_crawled'] for entry in recent_crawls)
    
    success_rate = (successful_crawls / total_attempts) * 100
    print(f"ìµœê·¼ 7ì¼ í¬ë¡¤ë§ ì„±ê³µë¥ : {success_rate:.1f}%")
```

### 2. íŠ¸ë Œë“œ ë¶„ì„

```python
# ì£¼ê°„ íŠ¸ë Œë”© í† í”½ ë¶„ì„
def analyze_weekly_trends():
    trend_counts = {}
    
    # ìµœê·¼ 7ì¼ ë°ì´í„° ë¶„ì„
    for date_folder in os.listdir('research/trends/'):
        if not date_folder.startswith('2024'):
            continue
            
        with open(f'research/trends/{date_folder}/crawling_results.json') as f:
            data = json.load(f)
            
        for topic in data.get('trending_topics', []):
            topic_name = topic['topic']
            trend_counts[topic_name] = trend_counts.get(topic_name, 0) + topic['count']
    
    # ìƒìœ„ 10ê°œ íŠ¸ë Œë“œ ì¶œë ¥
    sorted_trends = sorted(trend_counts.items(), key=lambda x: x[1], reverse=True)[:10]
    print("ì£¼ê°„ íŠ¸ë Œë”© í† í”½:")
    for topic, count in sorted_trends:
        print(f"  {topic}: {count}íšŒ ì–¸ê¸‰")
```

---

## ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œë“¤

#### 1. RSS í”¼ë“œ ì ‘ê·¼ ì‹¤íŒ¨
```python
# í•´ê²° ë°©ë²•: User-Agent í—¤ë” ì¶”ê°€
headers = {
    'User-Agent': 'Mozilla/5.0 (compatible; TechCrawler/1.0)',
    'Accept': 'application/rss+xml, application/xml, text/xml'
}
```

#### 2. API ìœ¨ì œí•œ (Rate Limiting)
```python
# í•´ê²° ë°©ë²•: ì§€ìˆ˜ ë°±ì˜¤í”„ì™€ ì¬ì‹œë„
import asyncio
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
async def api_request_with_retry(url):
    async with session.get(url) as response:
        if response.status == 429:  # Too Many Requests
            raise Exception("Rate limited")
        return await response.json()
```

#### 3. ì»¨í…ì¸  íŒŒì‹± ì‹¤íŒ¨
```python
# í•´ê²° ë°©ë²•: ë‹¤ì¤‘ ì…€ë ‰í„° ì‚¬ìš©
def extract_content_safely(soup, selectors):
    for selector in selectors:
        try:
            element = soup.select_one(selector)
            if element and element.get_text(strip=True):
                return element.get_text(strip=True)
        except Exception:
            continue
    return ""
```

### ë¡œê·¸ í™•ì¸ ë°©ë²•

```bash
# GitHub Actions ë¡œê·¸
# Repository > Actions > Daily Tech Blog Crawler

# ë¡œì»¬ ë¡œê·¸ íŒŒì¼
tail -f automation/logs/crawler.log

# í¬ë¡¤ë§ í†µê³„ í™•ì¸
cat research/trends/index.json | jq '.statistics'
```

---

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™”

### 1. ë³‘ë ¬ ì²˜ë¦¬
```python
# ë¸”ë¡œê·¸ë³„ ë™ì‹œ í¬ë¡¤ë§
async def crawl_all_blogs_parallel():
    tasks = []
    for blog_name, config in BLOG_SOURCES.items():
        task = asyncio.create_task(crawler.crawl_blog(blog_name, config))
        tasks.append(task)
    
    results = await asyncio.gather(*tasks, return_exceptions=True)
```

### 2. ìºì‹± ì „ëµ
```python
# ì»¨í…ì¸  ìºì‹±ìœ¼ë¡œ ì¤‘ë³µ ìš”ì²­ ë°©ì§€
def get_cached_content(url, cache_duration=3600):
    cache_key = hashlib.md5(url.encode()).hexdigest()
    cache_file = Path(f"cache/{cache_key}.txt")
    
    if cache_file.exists():
        if time.time() - cache_file.stat().st_mtime < cache_duration:
            return cache_file.read_text()
    
    return None
```

### 3. ë©”ëª¨ë¦¬ ê´€ë¦¬
```python
# ëŒ€ìš©ëŸ‰ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
async def process_large_feed(feed_url):
    async with session.get(feed_url) as response:
        async for chunk in response.content.iter_chunked(8192):
            # ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
            process_chunk(chunk)
```

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2024ë…„ í˜„ì¬  
**ë²„ì „**: 2.0.0  
**ë¼ì´ì„ ìŠ¤**: MIT 