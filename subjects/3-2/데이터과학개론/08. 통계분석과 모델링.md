# 08. 통계분석과 모델링

## 학습 목표
## 학습 목표
1. **통계적 모델링의 이해**: 통계적 모델의 정의와 목적(설명·예측·데이터 구조 분석)을 설명하고, 모형의 **파라메트릭 vs 비파라메트릭**, **모형 가정(선형성·독립성·정규성 등)** 을 구분할 수 있다.
2. **확률변수와 확률분포**: 확률변수와 확률분포의 개념을 설명하고, **정규분포·이항분포·포아송분포**의 가정·특성·실사용 예시를 제시할 수 있다.
3. **기술통계와 추론통계 구분**: 기술통계(평균·중앙값·분산·사분위수·시각화)와 추론통계(표본·모집단·신뢰구간·추정) 의 역할을 비교하여 설명할 수 있다.
4. **가설검정의 절차와 해석**: 귀무/대립가설 설정, 유의수준(α), 검정통계량, p-value 해석, 검정력(power) 개념을 설명하고 **t-검정(독립·대응)** 을 사례로 절차와 결과 해석을 수행할 수 있다.
5. **상관분석 vs 회귀분석**: 상관계수의 의미(방향·강도)와 회귀분석의 목적(인과·예측) 차이를 설명하고, **단순선형회귀의 OLS 추정, 회귀계수 해석, 잔차분석 및 가정검토**를 수행할 수 있다.
6. **모델 성능 평가**: 결정계수(R²), RMSE, MAE 등 회귀 지표와 혼동행렬·정밀도·재현율·F1 등 분류 지표의 계산법과 해석, 그리고 목적에 따른 지표 선택 기준을 설명할 수 있다.
7. **실습 역량**: 실제 데이터에서 간단한 회귀·분류 모델을 적합하고, 결과를 시각화·해석하여 짧은 보고서(핵심 결과·한계·실무적 시사점)를 작성할 수 있다.
8. **AI 도구 활용 (NotebookLM 등)**: AI 학습 도우미에 효과적인 프롬프트를 작성하여 코드 생성·시각화·해석을 얻는 방법을 설명하고 실습할 수 있다.

**선수 지식**: 기초 확률·통계(평균·분산·확률), 선형대수(기본), Python 기초(`pandas`, `numpy`) 권장.

**권장 학습 시간**: 강의 1~2회분(약 2~4시간) + 실습 2시간 권장.

---

## 📝 요약 (Summary)
1.  **통계적 모델링**: 데이터를 잘 설명하고 미래를 예측하기 위해, 데이터가 특정 확률 분포를 따른다고 가정하고 수학적 모델(함수)로 표현하는 과정입니다.
2.  **확률 분포**: 특정 사건이 발생할 확률을 나타내는 함수입니다. 데이터의 특성에 따라 **정규분포**(연속형 데이터), **이항분포**(성공/실패 결과), **포아송분포**(단위 시간/공간 내 사건 발생 횟수) 등을 사용합니다.
3.  **기술통계 vs. 추론통계**:
    -   **기술통계**: 수집한 데이터를 요약하고 묘사하는 데 집중합니다. (예: 평균, 중앙값, 표준편차)
    -   **추론통계**: 표본(Sample) 데이터를 사용하여 모집단(Population)의 특성을 추정하고 가설을 검정합니다.
4.  **가설검정**: 표본을 통해 얻은 사실을 근거로, 모집단에 대한 가설이 맞는지 통계적으로 판단하는 과정입니다. **귀무가설(H0)** 과 **대립가설(H1)** 을 설정하고, **p-value**가 **유의수준(α)** 보다 작으면 귀무가설을 기각합니다.
5.  **상관분석 vs. 회귀분석**:
    -   **상관분석**: 두 변수 간의 선형적인 관계의 **방향성**과 **강도**를 측정합니다. (예: 키와 몸무게의 관계)
    -   **회귀분석**: 한 변수(독립변수)가 다른 변수(종속변수)에 미치는 영향을 파악하고, 이를 통해 값을 **예측**하는 모델을 만듭니다.

---
## ✏️ 심화 학습 (Study Subject)
1. **기술통계**와 **추론통계**의 근본적인 차이점은 무엇이며, "우리 반 학생들의 평균 키는 175cm이다"라는 문장과 "우리나라 20대 남성의 평균 키는 174cm일 것으로 95% 신뢰수준에서 추정된다"라는 문장이 각각 어디에 해당하는지 그 이유를 설명하시오.
2. **가설검정**에서 **귀무가설(H0)**과 **대립가설(H1)**을 설정하는 것이 왜 중요한지 설명하고, "새로 개발한 비료가 기존 비료보다 수확량을 유의미하게 증가시키는가?"를 검정하기 위한 가설을 구체적으로 설정하시오.
3. **p-value**가 0.03일 때, 유의수준(α)이 0.05인 경우와 0.01인 경우에 각각 어떤 통계적 결정을 내리게 되는지, 그리고 그 결정의 의미가 무엇인지 설명하시오.
4. **상관분석**과 **회귀분석**은 모두 변수 간의 관계를 다루지만, 그 목적과 해석에서 중요한 차이가 있다. '아이스크림 판매량'과 '기온' 사이의 관계를 예로 들어 두 분석의 차이점을 설명하시오.
5. 선형회귀모델의 성능을 평가하는 지표인 **결정계수(R²)**가 0.7이라는 것의 의미를 설명하고, R² 값이 높다고 해서 반드시 좋은 모델이라고 할 수 없는 이유(예: 모델의 가정 위배)를 설명하시오.

---

## ❓ 연습 문제 및 해설

### 📝 문제

**1. 다음 중 데이터를 2차원 표(Table) 형태로 다루고, CSV 파일 읽기, 데이터 선택, 집계 등 다양한 데이터 가공 기능을 제공하는 파이썬 라이브러리는?**
> ① NumPy
> ② Matplotlib
> ③ Pandas
> ④ Scikit-learn
**1. 다음 중 수집된 데이터의 특성을 요약하고 설명하는 데 중점을 두는 통계 분석 방법은?**
> ① 추론통계 (Inferential Statistics)
> ② 기술통계 (Descriptive Statistics)
> ③ 회귀분석 (Regression Analysis)
> ④ 가설검정 (Hypothesis Testing)

<br>

**2. 다음 중 파이썬에서 NumPy 배열로부터 Pandas의 DataFrame을 생성하는 코드로 가장 적절한 것은? (단, `import pandas as pd`, `import numpy as np` 실행 가정)**
> ① `pd.to_frame(np.array)`
> ② `pd.DataFrame(np.array([1,2,3]))`
> ③ `np.to_dataframe([1,2,3])`
> ④ `pd.Series(np.array([1,2,3]))`
**2. 어떤 약의 효과를 검증하기 위해 "새로운 약은 효과가 없다"라는 가설을 설정했다. 이 가설은 다음 중 무엇에 해당하는가?**
> ① 대립가설 (Alternative Hypothesis, H1)
> ② 연구가설 (Research Hypothesis)
> ③ 귀무가설 (Null Hypothesis, H0)
> ④ 기술가설 (Descriptive Hypothesis)

<br>

**3. 다음 파이썬 리스트 슬라이싱(Slicing) 코드의 실행 결과로 옳은 것은?**
> ```python
> my_list = [10, 20, 30, 40, 50]
> print(my_list[1:3])
> ```
> ① `[10, 20, 30]`
> ② `[20, 30]`
> ③ `[20, 30, 40]`
> ④ `[10, 20]`
**3. 가설검정 결과 p-value가 0.04로 계산되었다. 유의수준(α)을 5%(0.05)로 설정했을 때, 내려야 할 올바른 통계적 결론은?**
> ① 귀무가설을 채택한다.
> ② 대립가설을 기각한다.
> ③ 귀무가설을 기각한다.
> ④ 결론을 내릴 수 없다.

<br>

**4. Pandas DataFrame에서 명시적인 인덱스 '이름'(label)을 사용하여 행을 선택하려고 할 때, 가장 적절한 접근자는?**
> ① `df.iloc[...]`
> ② `df.loc[...]`
> ③ `df[...]`
> ④ `df.query(...)`
**4. 두 연속형 변수 간의 선형적인 관계의 '방향'과 '강도'를 측정하는 분석 방법은?**
> ① t-검정 (t-test)
> ② 분산분석 (ANOVA)
> ③ 회귀분석 (Regression Analysis)
> ④ 상관분석 (Correlation Analysis)

<br>

**5. 다음 Pandas 코드의 기능으로 가장 올바른 설명은?**
> ```python
> df.sample(frac=0.5)
> ```
> ① 데이터프레임에서 0.5라는 값을 가진 행을 추출한다.
> ② 데이터프레임의 모든 수치에 0.5를 곱한다.
> ③ 데이터프레임에서 50% 비율의 행을 무작위로 추출(샘플링)한다.
> ④ 데이터프레임의 상위 50% 행을 선택한다.
**5. "광고비 지출이 매출에 미치는 영향을 분석하여, 광고비 1억 원 증가 시 매출이 얼마나 증가할지 예측하는 모델"을 만들고자 할 때 가장 적합한 분석 방법은?**
> ① 상관분석
> ② 회귀분석
> ③ 기술통계분석
> ④ 교차분석

<br>

**6. Pandas의 `drop_duplicates()` 함수를 사용하는 주된 목적은?**
> ① 결측치(Missing Value)를 특정 값으로 채우기 위해
> ② 중복된 행(row)을 제거하기 위해
> ③ 특정 열(column)을 삭제하기 위해
> ④ 데이터의 순서를 정렬하기 위해
**6. 다음 중 특정 사건이 단위 시간 또는 단위 공간 내에서 발생하는 평균 횟수를 기반으로, 특정 횟수가 발생할 확률을 모델링하는 데 가장 적합한 확률분포는? (예: 1시간 동안 은행에 방문하는 고객 수)**
> ① 정규분포 (Normal Distribution)
> ② 이항분포 (Binomial Distribution)
> ③ 포아송분포 (Poisson Distribution)
> ④ 균등분포 (Uniform Distribution)

<br>

**7. 다음 Pandas 코드에서 `axis=1` 옵션의 의미로 가장 옳은 것은?**
> ```python
> df.apply(lambda x: x.max() - x.min(), axis=1)
> ```
> ① 함수가 각 **열(column)** 단위로 적용된다.
> ② 함수가 각 **행(row)** 단위로 적용된다.
> ③ 함수가 첫 번째 열에만 적용된다.
> ④ 함수가 첫 번째 행에만 적용된다.

<br>

**8. Pandas에서 '부서'별 '연봉'의 평균을 계산하고자 할 때, 가장 적절한 함수 조합은?**
> ① `sort_values()`와 `agg()`
> ② `groupby()`와 `agg()`
> ③ `merge()`와 `sort_index()`
> ④ `concat()`와 `mean()`

<br>

**9. 두 개의 DataFrame을 공통된 'ID' 열을 기준으로 SQL의 `INNER JOIN`과 같이 병합하려고 할 때, 가장 적절한 함수는?**
> ① `pd.concat([df1, df2])`
> ② `df1.append(df2)`
> ③ `pd.merge(df1, df2, on='ID', how='inner')`
> ④ `df1.join(df2)`

<br>

**10. NumPy에서 두 행렬 간의 수학적인 행렬 곱(matrix multiplication)을 계산하는 함수는?**
> ① `np.multiply()`
> ② `np.dot()` 또는 `np.matmul()`
> ③ `np.concatenate()`
> ④ `np.transpose()`

---

### 🔑 정답

1. ③
2. ②
3. ②
4. ②
5. ③
6. ②
7. ②
8. ②
9. ③
10. ②
4. ④
5. ②
6. ③

---

### 🧐 해설

<a id="prob-1"></a>
**1. 다음 중 데이터를 2차원 표(Table) 형태로 다루고, CSV 파일 읽기, 데이터 선택, 집계 등 다양한 데이터 가공 기능을 제공하는 파이썬 라이브러리는?**
> **정답: ③ Pandas**
> **해설**: **Pandas**는 파이썬에서 데이터 분석과 조작을 위해 가장 널리 사용되는 라이브러리입니다. DataFrame 객체를 통해 표 형태의 데이터를 직관적으로 다룰 수 있습니다.
> - ① NumPy: 수치 계산 및 다차원 배열 처리에 특화되어 있습니다.
> - ② Matplotlib: 데이터 시각화를 위한 라이브러리입니다.
> - ④ Scikit-learn: 머신러닝 알고리즘을 제공하는 라이브러리입니다.
**1. 다음 중 수집된 데이터의 특성을 요약하고 설명하는 데 중점을 두는 통계 분석 방법은?**
> **정답: ② 기술통계 (Descriptive Statistics)**
> **해설**: **기술통계**는 주어진 데이터를 요약, 정리, 시각화하여 데이터의 전반적인 특징을 파악하는 데 사용됩니다. 평균, 중앙값, 표준편차 계산 등이 여기에 속합니다.
> - ① **추론통계**: 표본 데이터를 사용하여 모집단의 특성을 추정하거나 가설을 검정하는 방법입니다.

<a id="prob-2"></a>
**2. 다음 중 파이썬에서 NumPy 배열로부터 Pandas의 DataFrame을 생성하는 코드로 가장 적절한 것은?**
> **정답: ② `pd.DataFrame(np.array([1,2,3]))`**
> **해설**: `pd.DataFrame()` 함수는 리스트, 딕셔너리, NumPy 배열 등 다양한 데이터 소스를 입력받아 DataFrame을 생성할 수 있습니다.
> - ④ `pd.Series()`는 1차원 데이터 구조인 Series를 생성합니다.
**2. 어떤 약의 효과를 검증하기 위해 "새로운 약은 효과가 없다"라는 가설을 설정했다. 이 가설은 다음 중 무엇에 해당하는가?**
> **정답: ③ 귀무가설 (Null Hypothesis, H0)**
> **해설**: **귀무가설(H0)**은 연구자가 직접적으로 검증하고자 하는 대상이며, 보통 '차이가 없다', '효과가 없다'와 같이 보수적이고 기존에 알려진 사실을 나타냅니다. 연구자는 수집된 데이터를 근거로 이 귀무가설이 기각될 만큼 충분히 증거가 있는지를 확인하게 됩니다. 반면, **대립가설(H1)**은 '효과가 있다', '차이가 있다'와 같이 연구자가 주장하고 싶은 내용입니다.

<a id="prob-3"></a>
**3. 다음 파이썬 리스트 슬라이싱(Slicing) 코드의 실행 결과로 옳은 것은?**
> **정답: ② `[20, 30]`**
> **해설**: 파이썬 슬라이싱 `[start:end]`는 `start` 인덱스부터 `end-1` 인덱스까지의 원소를 선택합니다. 따라서 `my_list[1:3]`은 인덱스 1(`20`)과 인덱스 2(`30`)를 선택하여 `[20, 30]`을 반환합니다.
**3. 가설검정 결과 p-value가 0.04로 계산되었다. 유의수준(α)을 5%(0.05)로 설정했을 때, 내려야 할 올바른 통계적 결론은?**
> **정답: ③ 귀무가설을 기각한다.**
> **해설**: 가설검정의 핵심 원칙은 **"p-value < 유의수준(α)"** 이면 귀무가설을 기각하고 대립가설을 채택하는 것입니다. 이 경우, p-value(0.04)가 유의수준(0.05)보다 작으므로, "효과가 없다"는 귀무가설을 기각하고 "효과가 있다"는 대립가설을 지지하게 됩니다.

<a id="prob-4"></a>
**4. Pandas DataFrame에서 명시적인 인덱스 '이름'(label)을 사용하여 행을 선택하려고 할 때, 가장 적절한 접근자는?**
> **정답: ② `df.loc[...]`**
> **해설**: **`loc`**은 레이블(label) 기반 인덱싱을 위해 사용됩니다. 인덱스 이름이나 열 이름을 직접 지정하여 데이터를 선택합니다. 반면, **`iloc`**은 0부터 시작하는 정수 위치(integer-location) 기반 인덱싱을 위해 사용됩니다.
**4. 두 연속형 변수 간의 선형적인 관계의 '방향'과 '강도'를 측정하는 분석 방법은?**
> **정답: ④ 상관분석 (Correlation Analysis)**
> **해설**: **상관분석**은 상관계수( 보통 -1에서 +1 사이의 값)를 통해 두 변수가 함께 증가하는지(양의 상관), 하나가 증가할 때 다른 하나는 감소하는지(음의 상관)의 **방향성**과 그 관계가 얼마나 강한지의 **강도**를 수치적으로 나타냅니다. 인과관계를 설명하지는 않습니다.

<a id="prob-5"></a>
**5. 다음 Pandas 코드의 기능으로 가장 올바른 설명은?**
> **정답: ③ 데이터프레임에서 50% 비율의 행을 무작위로 추출(샘플링)한다.**
> **해설**: `sample()` 함수는 데이터프레임에서 무작위로 샘플을 추출하는 기능을 합니다. `frac` 파라미터는 전체 데이터에서 추출할 비율을 의미하며, `frac=0.5`는 50%를 의미합니다.
**5. "광고비 지출이 매출에 미치는 영향을 분석하여, 광고비 1억 원 증가 시 매출이 얼마나 증가할지 예측하는 모델"을 만들고자 할 때 가장 적합한 분석 방법은?**
> **정답: ② 회귀분석**
> **해설**: **회귀분석**은 독립변수(광고비)가 종속변수(매출)에 미치는 영향을 파악하고, 이를 수학적 모델(회귀식)로 표현하여 미래 값을 **예측**하는 데 사용됩니다. 이는 단순히 두 변수 간의 관계 강도만 보는 상관분석보다 한 단계 더 나아간 분석입니다.

<a id="prob-6"></a>
**6. Pandas의 `drop_duplicates()` 함수를 사용하는 주된 목적은?**
> **정답: ② 중복된 행(row)을 제거하기 위해**
> **해설**: `drop_duplicates()` 함수는 데이터셋에 존재하는 완전히 동일한 행들을 식별하고 제거하여 데이터의 정합성을 높이는 데 사용됩니다.
**6. 다음 중 특정 사건이 단위 시간 또는 단위 공간 내에서 발생하는 평균 횟수를 기반으로, 특정 횟수가 발생할 확률을 모델링하는 데 가장 적합한 확률분포는? (예: 1시간 동안 은행에 방문하는 고객 수)**
> **정답: ③ 포아송분포 (Poisson Distribution)**
> **해설**: **포아송분포**는 정해진 시간이나 공간 안에서 어떤 사건이 평균적으로 몇 번 발생하는지 알고 있을 때, 실제 그 사건이 특정 횟수(k번) 발생할 확률을 나타내는 이산 확률분포입니다.
> - ① **정규분포**: 평균을 중심으로 좌우대칭의 종 모양을 가지는 연속 확률분포입니다.
> - ② **이항분포**: 정해진 횟수의 독립적인 시도에서 성공/실패와 같이 두 가지 결과만 나올 때, 특정 성공 횟수가 나타날 확률을 나타냅니다.

<a id="prob-7"></a>
**7. 다음 Pandas 코드에서 `axis=1` 옵션의 의미로 가장 옳은 것은?**
> **정답: ② 함수가 각 **행(row)** 단위로 적용된다.**
> **해설**: Pandas에서 `axis` 옵션은 연산이 적용될 축을 지정합니다.
> - `axis=0` (기본값): 열(column) 방향으로 연산을 수행합니다. 함수는 각 열(Series)을 인자로 받습니다.
> - `axis=1`: 행(row) 방향으로 연산을 수행합니다. 함수는 각 행(Series)을 인자로 받습니다.

<a id="prob-8"></a>
**8. Pandas에서 '부서'별 '연봉'의 평균을 계산하고자 할 때, 가장 적절한 함수 조합은?**
> **정답: ② `groupby()`와 `agg()`**
> **해설**: `df.groupby('부서')`를 사용하여 데이터를 '부서'별로 그룹화한 다음, `agg({'연봉': 'mean'})` 또는 `.연봉.mean()`을 사용하여 각 그룹의 '연봉' 평균을 계산할 수 있습니다. 이는 데이터 집계의 가장 일반적인 패턴입니다.

<a id="prob-9"></a>
**9. 두 개의 DataFrame을 공통된 'ID' 열을 기준으로 SQL의 `INNER JOIN`과 같이 병합하려고 할 때, 가장 적절한 함수는?**
> **정답: ③ `pd.merge(df1, df2, on='ID', how='inner')`**
> **해설**: `pd.merge()` 함수는 두 DataFrame 간의 관계형 조인을 수행하는 데 사용됩니다. `on` 파라미터로 기준 열을 지정하고, `how` 파라미터로 `inner`, `left`, `right`, `outer` 등 조인 방식을 지정할 수 있습니다.

<a id="prob-10"></a>
**10. NumPy에서 두 행렬 간의 수학적인 행렬 곱(matrix multiplication)을 계산하는 함수는?**
> **정답: ② `np.dot()` 또는 `np.matmul()`**
> **해설**: `np.dot()`과 `np.matmul()` (또는 `@` 연산자)은 모두 행렬 곱을 계산하는 데 사용됩니다.
> - ① `np.multiply()`는 행렬의 같은 위치에 있는 원소끼리 곱하는 '요소별 곱(element-wise multiplication)'을 수행합니다.

---

## ✅ O/X 확인문제(연습문항 개수의 2배)

**1.** Pandas의 `Series`는 2차원 표 형태의 데이터 구조이다. **(O / X)**
**2.** [Pandas DataFrame에서 `df.loc[0]`은 항상 첫 번째 행을 반환한다.](#ox-2) **(O / X)**
**3.** NumPy는 파이썬에서 과학적 계산을 위한 핵심 라이브러리이다. **(O / X)**
**4.** Pandas의 `map` 함수는 DataFrame 전체에 함수를 적용할 때 사용한다. **(O / X)**
**5.** `df.groupby('column_A').sum()` 코드는 'column_A'를 기준으로 그룹화하여 각 그룹의 합계를 계산한다. **(O / X)**
**6.** `pd.concat()` 함수는 공통된 열(key)이 없으면 두 DataFrame을 병합할 수 없다. **(O / X)**
**7. ** NumPy 배열의 모든 원소는 동일한 데이터 타입을 가져야 한다. **(O / X)**
**8.** Pandas의 `df.describe()` 함수는 데이터의 기술 통계량 요약 정보를 보여준다. **(O / X)**
**9.** [`df.iloc[:, 0]` 코드는 DataFrame의 첫 번째 행(row)을 선택한다.](#ox-9) **(O / X)**
**10.** Pandas의 `apply` 함수에서 `axis=0`은 함수를 행(row) 단위로 적용하라는 의미이다. **(O / X)**
**11.** `pd.merge()`에서 `how='left'`는 왼쪽 DataFrame의 모든 행을 유지하면서 병합하는 것을 의미한다. **(O / X)**
**12.** NumPy의 `@` 연산자는 두 배열의 요소별 곱셈을 수행한다. **(O / X)**
**13.** [Pandas에서 `df['age'] > 30`과 같은 조건식을 사용하면 `True` 또는 `False` 값을 갖는 Boolean Series가 반환된다.](#ox-13) **(O / X)**
**14.** `df.dropna()`는 데이터프레임에서 중복된 행을 제거하는 함수이다. **(O / X)**
**15.** NumPy의 '브로드캐스팅(Broadcasting)'은 크기가 다른 배열 간의 연산을 가능하게 하는 기능이다. **(O / X)**
**16.** Pandas의 `pivot_table` 함수는 엑셀의 피벗 테이블과 유사한 기능을 제공한다. **(O / X)**
**17.** `df.loc`은 정수 위치 인덱스만 사용할 수 있다. **(O / X)**
**18.** Google Colab은 별도의 설치 없이 웹 브라우저에서 Pandas와 NumPy를 사용할 수 있는 환경을 제공한다. **(O / X)**


> <a id="ox-1"></a>
> **1. 정답**: X
> **해설**: `Series`는 1차원 배열 형태의 데이터 구조입니다. 2차원 표 형태의 데이터 구조는 `DataFrame`입니다.

> <a id="ox-2"></a>
> **2. 정답**: X
> **해설**: `df.loc[0]`은 인덱스 '레이블'이 0인 행을 찾습니다. 만약 인덱스가 문자열이거나 순서가 섞여 있다면 첫 번째 행이 아닐 수 있습니다. 위치 기반으로 첫 번째 행을 선택하려면 `df.iloc[0]`을 사용해야 합니다.

> <a id="ox-3"></a>
> **3. 정답**: O
> **해설**: NumPy는 다차원 배열 객체와 이를 다루는 다양한 함수들을 제공하여, 파이썬 기반 과학 계산의 근간을 이룹니다.

> <a id="ox-4"></a>
> **4. 정답**: X
> **해설**: `map` 함수는 `Series`의 각 원소에 함수나 딕셔너리를 적용할 때 사용합니다. DataFrame 전체에 적용하는 함수는 `applymap`입니다.

> <a id="ox-5"></a>
> **5. 정답**: O
> **해설**: `groupby()`는 데이터를 특정 기준으로 묶는 역할을 하며, 이후 집계 함수(`sum`, `mean`, `count` 등)를 통해 그룹별 통계를 계산할 수 있습니다.

> <a id="ox-6"></a>
> **6. 정답**: X
> **해설**: `concat()`은 기준 열 없이 단순히 두 DataFrame을 행이나 열 방향으로 이어 붙이는 함수입니다. 공통 열 기준 병합은 `merge()`의 역할입니다.

> <a id="ox-7"></a>
> **7. 정답**: O
> **해설**: NumPy 배열은 C언어 기반으로 구현되어 메모리 효율성과 연산 속도를 높이기 위해 모든 원소가 동일한 타입이어야 합니다.

> <a id="ox-8"></a>
> **8. 정답**: O
> **해설**: `describe()` 함수는 수치형 데이터 열에 대해 개수(count), 평균(mean), 표준편차(std), 최소/최대값, 사분위수 등을 한 번에 보여줍니다.

> <a id="ox-9"></a>
> **9. 정답**: X
> **해설**: `df.iloc[:, 0]`에서 `:`는 모든 행을 의미하고, `0`은 첫 번째 열을 의미합니다. 따라서 이 코드는 DataFrame의 첫 번째 **열(column)**을 선택합니다.

> <a id="ox-10"></a>
> **10. 정답**: X
> **해설**: `axis=0`은 열(column) 방향으로 연산을 적용하라는 의미입니다. 행 단위 적용은 `axis=1`입니다.

> <a id="ox-11"></a>
> **11. 정답**: O
> **해설**: `how='left'`는 SQL의 `LEFT JOIN`과 동일하게, 왼쪽 DataFrame을 기준으로 병합하며, 오른쪽 DataFrame에 일치하는 키가 없으면 `NaN`으로 채워집니다.

> <a id="ox-12"></a>
> **12. 정답**: X
> **해설**: `@` 연산자는 행렬 곱(matrix multiplication)을 수행합니다. 요소별 곱셈은 `*` 연산자 또는 `np.multiply()` 함수를 사용합니다.

> <a id="ox-13"></a>
> **13. 정답**: O
> **해설**: 이러한 Boolean Series를 사용하여 `df[df['age'] > 30]`과 같이 조건에 맞는 행을 필터링할 수 있습니다. 이를 'Boolean Indexing'이라고 합니다.

> <a id="ox-14"></a>
> **14. 정답**: X
> **해설**: `df.dropna()`는 결측치(NaN)가 포함된 행이나 열을 제거하는 함수입니다. 중복 행 제거는 `df.drop_duplicates()`입니다.

> <a id="ox-15"></a>
> **15. 정답**: O
> **해설**: 브로드캐스팅은 특정 조건 하에 NumPy가 자동으로 배열의 크기를 맞추어 연산을 수행하게 해주는 매우 강력하고 편리한 기능입니다.

> <a id="ox-16"></a>
> **16. 정답**: O
> **해설**: `pivot_table` 함수는 `index`, `columns`, `values`, `aggfunc` 등을 지정하여 데이터를 요약하고 재구조화하는 강력한 기능을 제공합니다.

> <a id="ox-17"></a>
> **17. 정답**: X
> **해설**: `df.loc`은 레이블(이름) 기반 인덱서입니다. 인덱스 이름이 정수일 수도 있지만, 문자열일 수도 있습니다. 정수 '위치'만 사용하는 것은 `df.iloc`입니다.

> <a id="ox-18"></a>
> **18. 정답**: O
> **해설**: Google Colab은 클라우드 기반의 Jupyter Notebook 환경으로, 데이터 과학에 필요한 주요 라이브러리들이 미리 설치되어 있어 편리합니다.

---

## 📖 심화 학습 예시 답안

#### 1. Pandas의 `loc`과 `iloc` 비교

Pandas에서 `loc`과 `iloc`은 DataFrame에서 데이터를 선택하는 가장 중요한 두 가지 방법이지만, 작동 방식에 명확한 차이가 있습니다.

| 구분 | `loc` (Label-based) | `iloc` (Integer-based) |
| :--- | :--- | :--- |
| **기반** | **레이블 (이름)** 기반 | **정수 위치 (0-based index)** 기반 |
| **선택 대상** | 인덱스 이름, 열 이름 | 0부터 시작하는 행/열의 정수 위치 |
| **슬라이싱** | `df.loc['a':'c']` -> 'a'부터 'c'까지 **모두 포함** | `df.iloc[0:2]` -> 0부터 1까지 **(2는 미포함)** |
| **사용 상황** | 인덱스나 열의 이름이 명확하고 의미 있을 때. 가독성이 중요할 때. | 인덱스 이름에 상관없이 순서상 특정 위치의 데이터를 가져오고 싶을 때. |

**예시 코드:**
```python
import pandas as pd

df = pd.DataFrame({'age': [25, 30, 22], 'score': [85, 90, 78]},
                  index=['Alice', 'Bob', 'Charlie'])
print(df)
#          age  score
# Alice     25     85
# Bob       30     90
# Charlie   22     78

# loc 예시: 이름으로 선택
print(df.loc['Bob']) # 'Bob' 행 선택
print(df.loc['Alice':'Bob', 'age']) # 'Alice'부터 'Bob'까지 행의 'age' 열 선택

# iloc 예시: 정수 위치로 선택
print(df.iloc[1]) # 1번 위치(두 번째) 행 선택
print(df.iloc[0:2, 0]) # 0~1번 위치 행의 0번 위치 열 선택
```

#### 2. `apply`, `map`, `applymap` 비교

Pandas에서 함수를 적용하는 이 세 가지 방법은 적용 대상과 범위에 따라 구분됩니다.

| 함수 | 적용 대상 | 설명 | 예시 |
| :--- | :--- | :--- | :--- |
| **`map`** | `Series` | Series의 각 **원소(element)**에 함수를 적용. 딕셔너리를 전달하여 값을 매핑하는 용도로도 자주 사용됨. | `df['gender_code'].map({0: 'Male', 1: 'Female'})` |
| **`apply`** | `DataFrame` 또는 `Series` | `axis` 설정에 따라 DataFrame의 **행(row) 또는 열(column)** 전체에 함수를 적용. `Series`에도 사용 가능. | `df[['kor', 'eng']].apply(sum, axis=0)` |
| **`applymap`** | `DataFrame` | DataFrame의 **모든 원소(element)** 각각에 함수를 적용. `Series`에는 사용할 수 없음. | `df.applymap(lambda x: f'Value: {x}')` |

**시나리오**: "학생들의 'score' 열에 대해, 90점 이상이면 'A', 80점 이상이면 'B', 그 외에는 'C' 등급을 부여하는 'grade' 열을 새로 만들고 싶다."

이 경우, 'score'라는 **단일 열(Series)**에 복잡한 조건부 함수를 적용해야 하므로 **`apply`** 함수와 `lambda`를 사용하는 것이 가장 적합하고 가독성이 좋습니다.

```python
def get_grade(score):
    if score >= 90:
        return 'A'
    elif score >= 80:
        return 'B'
    else:
        return 'C'

# Series에 apply 함수 사용
df['grade'] = df['score'].apply(get_grade)

# 또는 lambda 함수를 직접 사용
# df['grade'] = df['score'].apply(lambda x: 'A' if x >= 90 else ('B' if x >= 80 else 'C'))
```
`map`은 주로 1:1 매핑에, `applymap`은 모든 셀에 동일 서식 적용 등 일괄 변환에 더 적합합니다.

#### 3. SQL JOIN과 Pandas `merge` 비교

Pandas의 `merge` 함수는 SQL의 `JOIN` 구문과 매우 유사하게 동작하며, `how` 파라미터를 통해 조인 방식을 지정할 수 있습니다.

| SQL JOIN | Pandas `merge(..., how=...)` | 설명 |
| :--- | :--- | :--- |
| `INNER JOIN` | `how='inner'` (기본값) | 두 DataFrame에 공통으로 존재하는 키만 병합 |
| `LEFT OUTER JOIN` | `how='left'` | 왼쪽 DataFrame의 모든 키를 유지하며 병합 |
| `RIGHT OUTER JOIN` | `how='right'` | 오른쪽 DataFrame의 모든 키를 유지하며 병합 |
| `FULL OUTER JOIN` | `how='outer'` | 양쪽 DataFrame의 모든 키를 유지하며 병합 |

**예시**: '고객' DataFrame과 '주문' DataFrame을 `customer_id`를 기준으로 병합하기

```python
customers = pd.DataFrame({'customer_id': [1, 2, 3], 'name': ['Alice', 'Bob', 'Charlie']})
orders = pd.DataFrame({'order_id': [101, 102, 103], 'customer_id': [1, 2, 4], 'item': ['Book', 'Pen', 'Eraser']})

# INNER JOIN: 주문 내역이 있는 고객(Alice, Bob)만 병합
inner_join = pd.merge(customers, orders, on='customer_id', how='inner')

# LEFT JOIN: 모든 고객 정보를 유지하고, 주문 내역이 없는 고객(Charlie)은 NaN으로 표시
left_join = pd.merge(customers, orders, on='customer_id', how='left')

print(left_join)
#    customer_id     name  order_id    item
# 0            1    Alice     101.0    Book
# 1            2      Bob     102.0     Pen
# 2            3  Charlie       NaN     NaN
```

#### 4. NumPy의 벡터화(Vectorization)와 성능

**벡터화**란, 반복문을 사용하지 않고 배열 전체에 대한 연산을 한 번에 수행하는 것을 의미합니다. NumPy의 벡터화 연산이 파이썬의 `for` 반복문보다 훨씬 빠른 이유는 다음과 같습니다.

1.  **컴파일된 코드 실행**: NumPy의 핵심 연산은 C, C++, 포트란 등 저수준 언어로 작성되어 미리 컴파일되어 있습니다. 파이썬의 인터프리터가 한 줄씩 코드를 해석하고 실행하는 것보다 훨씬 빠릅니다.
2.  **메모리 효율성**: NumPy의 `ndarray`는 모든 원소가 동일한 타입이며, 메모리에 연속적으로 저장됩니다. 이 구조는 CPU가 데이터를 효율적으로 캐싱하고 처리(SIMD 연산 등)하는 데 최적화되어 있습니다.
3.  **반복 오버헤드 감소**: 파이썬의 `for` 루프는 반복할 때마다 타입 체크, 인덱스 검사 등 부가적인 작업(오버헤드)이 발생하지만, NumPy는 이런 작업을 C 레벨에서 한 번에 처리합니다.

**성능 비교 코드:**
```python
import numpy as np
import time

size = 1_000_000

# NumPy 벡터화 연산
arr1 = np.arange(size)
arr2 = np.arange(size)
start_time = time.time()
result_np = arr1 + arr2
end_time = time.time()
print(f"NumPy 벡터화: {end_time - start_time:.5f} 초")

# 파이썬 for 반복문
list1 = list(range(size))
list2 = list(range(size))
result_py = []
start_time = time.time()
for i in range(size):
    result_py.append(list1[i] + list2[i])
end_time = time.time()
print(f"Python for 루프: {end_time - start_time:.5f} 초")
```
위 코드를 실행하면 NumPy 연산이 `for` 루프에 비해 수십 배에서 수백 배까지 빠른 것을 확인할 수 있습니다.

---

## 🌐 최신 동향 및 추가 정보

### 1. Pandas 2.0과 Apache Arrow
최근 릴리즈된 Pandas 2.0은 내부 데이터 처리 엔진으로 **Apache Arrow**를 선택적으로 사용할 수 있게 되면서 큰 성능 향상을 이루었습니다. Arrow는 다양한 시스템 간에 데이터를 빠르게 주고받기 위한 표준 인메모리(in-memory) 형식으로, 이를 백엔드로 사용하면 데이터 로딩, 처리 속도가 크게 개선되고 메모리 사용량도 줄어듭니다.
```python
# Arrow 백엔드를 사용하는 DataFrame 생성
df_arrow = pd.DataFrame({'a': [1, 2, 3]}, dtype='int64[pyarrow]')
```

### 2. Polars: 더 빠른 대안의 부상
**Polars**는 Rust 언어로 작성된 매우 빠른 데이터프레임 라이브러리로, Pandas의 대안으로 주목받고 있습니다. 멀티코어 CPU를 최대한 활용하는 병렬 처리와 효율적인 쿼리 최적화 엔진을 통해 대용량 데이터 처리에서 Pandas보다 월등한 성능을 보여줍니다. Pandas와 유사한 API를 제공하여 기존 Pandas 사용자도 쉽게 배울 수 있다는 장점이 있습니다.

### 3. 자연어 기반 데이터 분석: `pandas-ai`
생성형 AI 기술의 발전으로, 복잡한 Pandas 코드를 직접 작성하는 대신 자연어로 질문하여 데이터 분석을 수행하는 라이브러리도 등장했습니다. `pandas-ai`는 "가장 많이 팔린 상품 5개를 보여줘"와 같은 자연어 프롬프트를 입력하면, 이를 Pandas 코드로 변환하여 실행하고 결과를 반환해주는 기능을 제공하여 데이터 분석의 진입 장벽을 낮추고 있습니다.

---

## 📚 핵심 용어집 (Glossary)

- **Pandas**: 파이썬에서 표 형태의 데이터를 쉽게 가공하고 분석할 수 있도록 `DataFrame`과 `Series` 객체를 제공하는 핵심 데이터 분석 라이브러리.
- **DataFrame**: 행(row)과 열(column)으로 구성된 2차원 표 형태의 데이터 구조. Pandas에서 가장 기본이 되는 데이터 객체.
- **Series**: DataFrame의 각 열을 구성하는 1차원 배열 형태의 데이터 구조. 인덱스(index)와 값(value)으로 구성됨.
- **`loc`**: 레이블(이름) 기반으로 데이터에 접근하는 인덱서. `df.loc['row_label', 'col_label']`.
- **`iloc`**: 정수 위치 기반으로 데이터에 접근하는 인덱서. `df.iloc[row_position, col_position]`.
- **`groupby`**: 데이터를 특정 기준에 따라 여러 그룹으로 분할(Split)하는 연산. 이후 적용(Apply) 및 조합(Combine) 단계와 함께 사용됨.
- **`merge`**: 두 DataFrame을 특정 공통 열(key)을 기준으로 SQL의 JOIN처럼 병합하는 함수.
- **`concat`**: 여러 DataFrame을 행 또는 열 방향으로 단순히 이어 붙이는 함수.
- **NumPy**: 파이썬에서 다차원 배열 `ndarray`를 사용하여 효율적인 수치 계산을 수행하도록 지원하는 과학 계산의 근간이 되는 라이브-러리.
- **`ndarray` (N-dimensional array)**: NumPy의 핵심 데이터 구조로, 동일한 타입의 원소들로 구성된 다차원 배열.
- **벡터화 (Vectorization)**: 반복문을 사용하지 않고 배열 전체에 대한 연산을 수행하여 코드의 가독성과 실행 속도를 높이는 기법.
- **브로드캐스팅 (Broadcasting)**: NumPy에서 크기가 다른 배열 간에도 특정 조건 하에 연산이 가능하도록 배열을 자동으로 확장시켜주는 기능.

---

## 🤖 AI 학습 파트너를 위한 추가 자료

### 1. 가상 시나리오: "온라인 상점 판매 데이터 분석"

> **상황**: 당신은 온라인 상점의 데이터 분석가입니다. 'sales.csv' 파일에는 '날짜', '카테고리', '상품명', '수량', '가격' 열이 있습니다. CEO가 "가장 매출이 높은 상위 3개 카테고리는 무엇이며, 월별 총 매출 추이는 어떻게 되나요?"라고 질문했습니다.

**Q&A 기반 문제 해결 과정**
- **Q1. 먼저 무엇을 해야 할까요?**
  - **A.** `pd.read_csv('sales.csv')`를 사용하여 데이터를 DataFrame으로 불러옵니다. 그 다음, '수량'과 '가격' 열을 곱하여 '매출'이라는 새로운 파생 변수(열)를 `mutate`(`df['매출'] = df['수량'] * df['가격']`)를 통해 만들어야 합니다.

- **Q2. 카테고리별 매출 합계는 어떻게 계산하나요?**
  - **A.** `groupby()`와 `sum()`을 사용합니다. `df.groupby('카테고리')['매출'].sum()` 코드는 데이터를 '카테고리'별로 묶은 뒤, 각 그룹의 '매출' 합계를 계산합니다. 그 결과를 `sort_values(ascending=False)`로 정렬하고 `head(3)`으로 상위 3개를 추출할 수 있습니다.

- **Q3. 월별 매출 추이는 어떻게 분석하나요?**
  - **A.** 먼저 '날짜' 열을 `pd.to_datetime()`을 사용하여 날짜 타입으로 변환해야 합니다. 그 다음, `df['월'] = df['날짜'].dt.month` 코드로 '월' 정보를 추출합니다. 마지막으로 `df.groupby('월')['매출'].sum()`을 계산하고, 이 결과를 선 그래프로 시각화하면 월별 추이를 명확하게 파악할 수 있습니다.

### 2. 오개념 바로잡기 (Common Misconceptions)

- **오개념 1: "Pandas DataFrame은 파이썬의 기본 딕셔너리 리스트와 비슷하다."**
  - **바로잡기**: 구조는 비슷해 보이지만, 성능과 기능 면에서 큰 차이가 있습니다. DataFrame은 내부적으로 NumPy 배열을 사용하여 데이터를 메모리에 효율적으로 저장하고, C로 구현된 빠른 벡터화 연산을 제공합니다. 또한, 인덱싱, 집계, 병합, 시각화 등 파이썬 기본 자료구조에는 없는 강력하고 편리한 데이터 분석 전용 기능들을 제공합니다.

- **오개념 2: "데이터를 수정할 때 원본 DataFrame이 항상 바뀐다."**
  - **바로잡기**: Pandas의 많은 함수들은 기본적으로 **새로운 DataFrame을 반환**하고 원본은 변경하지 않습니다. 예를 들어, `df.dropna()`는 결측치가 제거된 '복사본'을 반환합니다. 원본을 직접 수정하고 싶다면 `inplace=True` 옵션을 사용해야 합니다. (예: `df.dropna(inplace=True)`). 이 동작 방식은 데이터 처리 과정에서 의도치 않은 원본 훼손을 방지하는 안전장치 역할을 합니다.

- **오개념 3: "NumPy와 Pandas는 서로 대체할 수 있다."**
  - **바로잡기**: 두 라이브러리는 상호 보완적인 관계입니다. NumPy는 순수한 **수치 계산**과 다차원 배열 처리에 집중하는 저수준 도구에 가깝습니다. 반면, Pandas는 NumPy를 기반으로 하여, 레이블이 있는 이종(heterogeneous) 데이터를 다루는 **데이터 분석**에 특화된 고수준 도구입니다. 복잡한 수학 연산은 NumPy로, 표 형태 데이터의 가공 및 분석은 Pandas로 수행하는 것이 일반적입니다.

---

## 📚 참고 자료 (References)

- Pandas 공식 문서 (10 minutes to pandas): Pandas의 핵심 기능을 10분 안에 빠르게 훑어볼 수 있는 공식 가이드입니다.
- NumPy 공식 튜토리얼 (Absolute basics for beginners): NumPy를 처음 접하는 사용자를 위한 공식 입문 가이드입니다.
- Python for Data Analysis, 3rd Edition: Pandas를 개발한 Wes McKinney가 직접 저술한 데이터 분석 필독서입니다.
- Kaggle Courses - Pandas: 실제 데이터를 다루며 Pandas를 연습할 수 있는 인터랙티브한 무료 온라인 강좌입니다.

